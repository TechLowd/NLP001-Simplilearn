{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello Mr. Smith, how are you doing today? \\\n",
    "        The weather is great, and Python is awesome!\\\n",
    "        The sky is pinkish-blue. \\\n",
    "        You shouldn\\'t eat cardboard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur felt very good. But he didn't go to play\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Mr. Smith, how are you doing today?',\n",
       " 'The weather is great, and Python is awesome!',\n",
       " 'The sky is pinkish-blue.',\n",
       " \"You shouldn't eat cardboard.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr. Smith, how are you doing today?\n",
      "The weather is great, and Python is awesome!\n",
      "The sky is pinkish-blue.\n",
      "You shouldn't eat cardboard.\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(text):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokens created using `sent_tokenize` is list of sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Arthur',\n",
       " 'felt',\n",
       " 'very',\n",
       " 'good',\n",
       " '.',\n",
       " 'But',\n",
       " 'he',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'go',\n",
       " 'to',\n",
       " 'play']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At\n",
      "eight\n",
      "o'clock\n",
      "on\n",
      "Thursday\n",
      "morning\n",
      "Arthur\n",
      "felt\n",
      "very\n",
      "good\n",
      ".\n",
      "But\n",
      "he\n",
      "did\n",
      "n't\n",
      "go\n",
      "to\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "for word in word_tokenize(sentence):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Mr.\n",
      "Smith\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      "doing\n",
      "today\n",
      "?\n",
      "The\n",
      "weather\n",
      "is\n",
      "great\n",
      ",\n",
      "and\n",
      "Python\n",
      "is\n",
      "awesome\n",
      "!\n",
      "The\n",
      "sky\n",
      "is\n",
      "pinkish-blue\n",
      ".\n",
      "You\n",
      "should\n",
      "n't\n",
      "eat\n",
      "cardboard\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in word_tokenize(text):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- Punctuations are treated as seperate token.\n",
    "- Notice the seperation of word \"shouldn't\" into \"should\" and \"n't\".\n",
    "- Notice that \"pinkish-blue\" is treated as one word.\n",
    "- Some words seems trival which form stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"this is Ram's text, is'nt it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', \"Ram's\", 'text,', \"is'nt\", 'it?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer.tokenize(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'Ram', \"'s\", 'text', ',', \"is'nt\", 'it', '?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'Ram', \"'\", 's', 'text', ',', 'is', \"'\", 'nt', 'it', '?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer.tokenize(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and', 'me', 'by', 'same', 'hers', 'doing', 'them', 'those', 'being', 'off', \"hadn't\", 'am', 'here', 'few', 'than', 'of', \"weren't\", 'is', 'a', 'should', \"should've\", \"doesn't\", 'other', 'did', 'below', 'how', 'has', 'd', 'both', 'our', \"shouldn't\", 'had', 'too', 'only', 'm', 'you', 'against', 'shouldn', 'most', 'my', 'yourselves', 'again', \"don't\", 'don', 'mustn', 'y', 'ain', 'needn', 'on', 'between', 'themselves', 'mightn', \"you'd\", 'they', \"she's\", 'in', \"you'll\", \"didn't\", 'why', 'was', 'for', 'who', \"aren't\", 'this', 'i', 'any', 'until', 'about', 'further', 'from', 'under', 'nor', 'to', 'o', 'each', 'been', 'very', 'can', 'does', 'didn', 'isn', 'do', \"mightn't\", 'yourself', 'the', 'theirs', \"mustn't\", \"won't\", 'hasn', 'your', \"hasn't\", 'haven', 'her', 'where', 'not', \"you've\", 'himself', 'be', 'wouldn', 'which', 'he', 'whom', 'weren', 'so', 'hadn', 'some', 't', \"wasn't\", \"isn't\", 'she', 'yours', 'an', 'its', 'if', 'after', 'aren', 'now', \"you're\", 'his', 'more', 'just', 'll', 'what', 'myself', 'doesn', 'then', 'such', \"shan't\", 'wasn', 'will', 'ma', \"that'll\", 'at', \"needn't\", 'above', 'as', 'out', 'when', 'with', 'it', 'herself', 'because', 'all', 'won', 'up', 'before', 'through', 'while', 'into', 'itself', 'their', 'over', 'having', \"haven't\", 'during', 'couldn', 'ourselves', 're', 'were', 'we', 'own', 'no', 'or', \"wouldn't\", 'shan', 'once', 'are', \"couldn't\", 'down', 'there', 'him', 'have', 've', \"it's\", 'but', 'that', 's', 'ours', 'these'}\n"
     ]
    }
   ],
   "source": [
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_sentence = \"This is a sample sentence, showing off the stop words filteration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(ex_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filteration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filteration', '.']\n"
     ]
    }
   ],
   "source": [
    "# option 1\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "# option 2\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "            \n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming words\n",
    "\n",
    "The idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other that when tense is involved.\n",
    "\n",
    "The reason why we stem is to shorten the lookup, and normalize sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstem = PorterStemmer()\n",
    "lstem = LancasterStemmer()\n",
    "sstem = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 PorterStemmer        LancasterStemmer     SnowballStemmer     \n",
      "connected            connect              connect              connect             \n",
      "connecting           connect              connect              connect             \n",
      "connection           connect              connect              connect             \n",
      "connections          connect              connect              connect             \n"
     ]
    }
   ],
   "source": [
    "word_list = ['connected', 'connecting', 'connection', 'connections']\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {3:20}\".format(\"Word\",\n",
    "                                           \"PorterStemmer\",\n",
    "                                           \"LancasterStemmer\",\n",
    "                                           \"SnowballStemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {3:20}\".format(word,\n",
    "                                               pstem.stem(word),\n",
    "                                               lstem.stem(word),\n",
    "                                               sstem.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            PorterStemmer   LancasterStemmer SnowballStemmer\n",
      "run             run             run             run            \n",
      "running         run             run             run            \n",
      "runs            run             run             run            \n",
      "runner          runner          run             runner         \n",
      "monthly         monthli         month           month          \n"
     ]
    }
   ],
   "source": [
    "word_list = ['run', 'running', 'runs', 'runner', 'monthly']\n",
    "\n",
    "print(\"{0:15} {1:15} {2:15} {3:15}\".format('Word',\n",
    "                                           'PorterStemmer',\n",
    "                                           'LancasterStemmer',\n",
    "                                           'SnowballStemmer'))\n",
    "for word in word_list:\n",
    "    print(\"{0:15} {1:15} {2:15} {3:15}\".format(word,\n",
    "                                               pstem.stem(word), \n",
    "                                               lstem.stem(word),\n",
    "                                               sstem.stem(word))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            PorterStemmer   LancasterStemmer SnowballStemmer\n",
      "cats            cat             cat             cat            \n",
      "touble          toubl           toubl           toubl          \n",
      "troubling       troubl          troubl          troubl         \n",
      "troubled        troubl          troubl          troubl         \n",
      "troublesome     troublesom      troublesom      troublesom     \n"
     ]
    }
   ],
   "source": [
    "word_list = ['cats', 'touble', 'troubling', 'troubled', 'troublesome']\n",
    "\n",
    "print(\"{0:15} {1:15} {2:15} {3:15}\".format('Word',\n",
    "                                          'PorterStemmer',\n",
    "                                          'LancasterStemmer',\n",
    "                                          'SnowballStemmer'))\n",
    "for word in word_list:\n",
    "    print(\"{0:15} {1:15} {2:15} {3:15}\".format(word,\n",
    "                                               pstem.stem(word),\n",
    "                                               lstem.stem(word),\n",
    "                                               sstem.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            PorterStemmer   LancasterStemmer SnowballStemmer\n",
      "argue           argu            argu            argu           \n",
      "argued          argu            argu            argu           \n",
      "argues          argu            argu            argu           \n",
      "arguing         argu            argu            argu           \n",
      "argus           argu            arg             argus          \n"
     ]
    }
   ],
   "source": [
    "word_list = ['argue', 'argued', 'argues', 'arguing', 'argus']\n",
    "\n",
    "print(\"{0:15} {1:15} {2:15} {3:15}\".format('Word',\n",
    "                                           'PorterStemmer',\n",
    "                                           'LancasterStemmer',\n",
    "                                           'SnowballStemmer'))\n",
    "for word in word_list:\n",
    "    print(\"{0:15} {1:15} {2:15} {3:15}\".format(word,\n",
    "                                               pstem.stem(word),\n",
    "                                               lstem.stem(word),\n",
    "                                               sstem.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            PorterStemmer   LancasterStemmer SnowballStemmer\n",
      "friend          friend          friend          friend         \n",
      "friendship      friendship      friend          friendship     \n",
      "friends         friend          friend          friend         \n",
      "friendships     friendship      friend          friendship     \n",
      "stabil          stabil          stabl           stabil         \n",
      "destablise      destablis       dest            destablis      \n",
      "misunderstanding misunderstand   misunderstand   misunderstand  \n",
      "railroad        railroad        railroad        railroad       \n",
      "moonlight       moonlight       moonlight       moonlight      \n",
      "football        footbal         footbal         footbal        \n",
      "cricket         cricket         cricket         cricket        \n",
      "cycle           cycl            cyc             cycl           \n",
      "rained          rain            rain            rain           \n"
     ]
    }
   ],
   "source": [
    "word_list = ['friend', 'friendship', 'friends', 'friendships', 'stabil',\n",
    "            'destablise', 'misunderstanding', 'railroad', 'moonlight',\n",
    "            'football', 'cricket', 'cycle', 'rained']\n",
    "\n",
    "print('{0:15} {1:15} {2:15} {3:15}'.format('Word',\n",
    "                                           'PorterStemmer',\n",
    "                                           'LancasterStemmer',\n",
    "                                           'SnowballStemmer'))\n",
    "for word in word_list:\n",
    "    print('{0:15} {1:15} {2:15} {3:15}'.format(word,\n",
    "                                              pstem.stem(word),\n",
    "                                              lstem.stem(word),\n",
    "                                              sstem.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is the process of converting a word to its base form.\n",
    "\n",
    "The difference between stemming and lemmatization is\n",
    "> Lemmatization considers the context and converts the words to its meaning ful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n",
    "\n",
    "There are multiple ways to lemmatization such as\n",
    "> - Wordnet Lemmatizer\n",
    "> - Spacy Lemmatizer\n",
    "> - TextBlob\n",
    "> - CLiPs Pattern\n",
    "> - Stanford CoreNLP\n",
    "> - Gensim Lemmatizer\n",
    "> - TreeTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# initiate WordNetLemmatizer class object\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 WordNetLemmatizer   \n",
      "friend               friend              \n",
      "friendship           friendship          \n",
      "eaten                eaten               \n",
      "bicycle              bicycle             \n",
      "fatten               fatten              \n",
      "player               player              \n",
      "introducing          introducing         \n",
      "datum                datum               \n",
      "data                 data                \n",
      "processing           processing          \n"
     ]
    }
   ],
   "source": [
    "word_list = ['friend', 'friendship', 'eaten', 'bicycle', 'fatten',\n",
    "             'player', 'introducing', 'datum', 'data', 'processing']\n",
    "\n",
    "print('{0:20} {1:20}'.format('Word', 'WordNetLemmatizer'))\n",
    "for word in word_list:\n",
    "    print('{0:20} {1:20}'.format(word, wnl.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The striped bats are hanging on their feet for best.'\n",
    "\n",
    "# tokenize\n",
    "word_list = word_tokenize(sentence)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The             The            \n",
      "striped         striped        \n",
      "bats            bat            \n",
      "are             are            \n",
      "hanging         hanging        \n",
      "on              on             \n",
      "their           their          \n",
      "feet            foot           \n",
      "for             for            \n",
      "best            best           \n",
      ".               .              \n"
     ]
    }
   ],
   "source": [
    "for word in word_list:\n",
    "    print('{0:15} {1:15}'.format(word, wnl.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: WordNetLemmatizers missing some words. To fix that words must be tagged with PoS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hang\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('hanging', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('eating', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('walking', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the N-grams for the given sentence\n",
    "\n",
    "The essential concept in text mining is n-grams, which are a set of co-occuring or continuous sequence of n items from a sequence of large text or sentence. The item here could be words, letters, and syllables. 1-gram is also called as unigrams are the unique words present in the sentence. Bigram (2-gram) is the combination of 2 words. Trigram (3-words) is 3 words and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Data science is an interesting field of study, includes ML and DL as sub field'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = 2\n",
    "\n",
    "n_grams = ngrams(nltk.word_tokenize(text), grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x0000027E16BB77C8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'science'),\n",
       " ('science', 'is'),\n",
       " ('is', 'an'),\n",
       " ('an', 'interesting'),\n",
       " ('interesting', 'field'),\n",
       " ('field', 'of'),\n",
       " ('of', 'study'),\n",
       " ('study', ','),\n",
       " (',', 'includes'),\n",
       " ('includes', 'ML'),\n",
       " ('ML', 'and'),\n",
       " ('and', 'DL'),\n",
       " ('DL', 'as'),\n",
       " ('as', 'sub'),\n",
       " ('sub', 'field')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.bigrams(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data science',\n",
       " 'science is',\n",
       " 'is an',\n",
       " 'an interesting',\n",
       " 'interesting field',\n",
       " 'field of',\n",
       " 'of study',\n",
       " 'study ,',\n",
       " ', includes',\n",
       " 'includes ML',\n",
       " 'ML and',\n",
       " 'and DL',\n",
       " 'DL as',\n",
       " 'as sub',\n",
       " 'sub field']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data science is a wonderful program, Data science is a land of opportunitites, Data science is about machine learning'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Data science is a wonderful program, Data science is a land of opportunitites, Data science is about machine learning'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Data', 'science'): 3,\n",
       "         ('science', 'is'): 3,\n",
       "         ('is', 'a'): 2,\n",
       "         ('a', 'wonderful'): 1,\n",
       "         ('wonderful', 'program'): 1,\n",
       "         ('program', ','): 1,\n",
       "         (',', 'Data'): 2,\n",
       "         ('a', 'land'): 1,\n",
       "         ('land', 'of'): 1,\n",
       "         ('of', 'opportunitites'): 1,\n",
       "         ('opportunitites', ','): 1,\n",
       "         ('is', 'about'): 1,\n",
       "         ('about', 'machine'): 1,\n",
       "         ('machine', 'learning'): 1})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram:  ['A', 'class', 'is', 'a', 'blueprint', 'for', 'the', 'object', '.']\n",
      "2-gram:  ['A class', 'class is', 'is a', 'a blueprint', 'blueprint for', 'for the', 'the object', 'object .']\n",
      "3-gram:  ['A class is', 'class is a', 'is a blueprint', 'a blueprint for', 'blueprint for the', 'for the object', 'the object .']\n",
      "4-gram:  ['A class is a', 'class is a blueprint', 'is a blueprint for', 'a blueprint for the', 'blueprint for the object', 'for the object .']\n"
     ]
    }
   ],
   "source": [
    "# function to generate n-grams from sentences\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(word_tokenize(data), num)\n",
    "    return [' ' .join(gram) for gram in n_grams]\n",
    "\n",
    "data = 'A class is a blueprint for the object.'\n",
    "\n",
    "print(\"1-gram: \", extract_ngrams(data, 1))\n",
    "print(\"2-gram: \", extract_ngrams(data, 2))\n",
    "print(\"3-gram: \", extract_ngrams(data ,3))\n",
    "print(\"4-gram: \", extract_ngrams(data, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'science', 'is'),\n",
       " ('science', 'is', 'a'),\n",
       " ('is', 'a', 'wonderful'),\n",
       " ('a', 'wonderful', 'program'),\n",
       " ('wonderful', 'program', ','),\n",
       " ('program', ',', 'Data'),\n",
       " (',', 'Data', 'science'),\n",
       " ('Data', 'science', 'is'),\n",
       " ('science', 'is', 'a'),\n",
       " ('is', 'a', 'land'),\n",
       " ('a', 'land', 'of'),\n",
       " ('land', 'of', 'opportunitites'),\n",
       " ('of', 'opportunitites', ','),\n",
       " ('opportunitites', ',', 'Data'),\n",
       " (',', 'Data', 'science'),\n",
       " ('Data', 'science', 'is'),\n",
       " ('science', 'is', 'about'),\n",
       " ('is', 'about', 'machine'),\n",
       " ('about', 'machine', 'learning')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Data science is a wonderful program, Data science is a land of opportunitites, Data science is about machine learning'\n",
    "\n",
    "list(nltk.trigrams(word_tokenize(text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
